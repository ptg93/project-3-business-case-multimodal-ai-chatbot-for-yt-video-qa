{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627db171-fcfa-4700-97c3-fa05945f8b18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import yt_dlp\n",
    "from moviepy.editor import VideoFileClip\n",
    "import whisper\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import ffmpeg\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever, RetrievalQA\n",
    "from langgraph.checkpoint import MemorySaver\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import Tool\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "from transformers import pipeline\n",
    "\n",
    "from pytube import YouTube\n",
    "import requests\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a60dbf0-1654-4c28-a786-4df9671abefa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "ELEVEN_API_KEY = os.getenv('ELEVEN_API_KEY')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
    "elif not LANGCHAIN_API_KEY:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY environment variable not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636d8cb0-0cc6-43e8-b123-61dfd65b0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Ironhack_Project3\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "\n",
    "# Set up directories\n",
    "os.makedirs('uploads', exist_ok=True)\n",
    "os.makedirs('downloads', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511b4c69-19a8-4968-9688-03527ecb55cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to handle file uploads\n",
    "# Function to handle file uploads\n",
    "# Helper function to encode image in base64\n",
    "# Helper function to encode image in base64\n",
    "def encode_image(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Function to handle file uploads\n",
    "def handle_file_upload():\n",
    "    uploader = widgets.FileUpload(accept='video/*', multiple=False)\n",
    "    display(uploader)\n",
    "    return uploader\n",
    "\n",
    "# Function to save uploaded file\n",
    "def save_uploaded_file(file):\n",
    "    start_time = time.time()\n",
    "    if file.value:\n",
    "        for file_info in file.value:\n",
    "            filename = file_info['name']\n",
    "            content = file_info['content']\n",
    "            print(f\"Filename: {filename}\")\n",
    "            file_path = os.path.join('uploads', filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(content)\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            print(f\"File {filename} uploaded successfully to {file_path} in {duration:.2f} seconds\")\n",
    "            return file_path, filename, duration\n",
    "    else:\n",
    "        print(\"No file uploaded.\")\n",
    "        return None, None, 0\n",
    "\n",
    "# Function to extract metadata using ffmpeg\n",
    "def extract_metadata_ffmpeg(video_path):\n",
    "    try:\n",
    "        probe = ffmpeg.probe(video_path)\n",
    "        video_info = next(stream for stream in probe['streams'] if stream['codec_type'] == 'video')\n",
    "        metadata = {\n",
    "            \"duration\": float(video_info['duration']),\n",
    "            \"width\": int(video_info['width']),\n",
    "            \"height\": int(video_info['height']),\n",
    "            \"codec_name\": video_info['codec_name'],\n",
    "            \"bit_rate\": int(video_info['bit_rate'])\n",
    "        }\n",
    "        print(f\"Extracted metadata: {metadata}\")\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata with ffmpeg: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to download video from YouTube\n",
    "def download_youtube_video(url):\n",
    "    start_time = time.time()\n",
    "    ydl_opts = {\n",
    "        'format': 'mp4',\n",
    "        'outtmpl': 'downloads/video.%(ext)s',\n",
    "        'verbose': True,\n",
    "    }\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info_dict = ydl.extract_info(url, download=True)\n",
    "            \n",
    "            metadata = {\n",
    "                \"title\": info_dict.get('title', 'video'),\n",
    "                \"id\": info_dict.get('id'),\n",
    "                \"duration\": info_dict.get('duration'),\n",
    "                \"upload_date\": info_dict.get('upload_date'),\n",
    "                \"uploader\": info_dict.get('uploader'),\n",
    "                \"uploader_id\": info_dict.get('uploader_id'),\n",
    "                \"view_count\": info_dict.get('view_count'),\n",
    "                \"like_count\": info_dict.get('like_count'),\n",
    "                \"dislike_count\": info_dict.get('dislike_count'),\n",
    "                \"average_rating\": info_dict.get('average_rating'),\n",
    "                \"age_limit\": info_dict.get('age_limit'),\n",
    "                \"categories\": \", \".join(info_dict.get('categories', [])),\n",
    "                \"tags\": \", \".join(info_dict.get('tags', [])),\n",
    "                \"ext\": info_dict.get('ext'),\n",
    "                \"thumbnail\": info_dict.get('thumbnail'),\n",
    "                \"description\": info_dict.get('description'),\n",
    "                \"channel\": info_dict.get('channel'),\n",
    "                \"channel_id\": info_dict.get('channel_id'),\n",
    "                \"is_live\": info_dict.get('is_live'),\n",
    "                \"release_date\": info_dict.get('release_date'),\n",
    "                \"availability\": info_dict.get('availability')\n",
    "            }\n",
    "            \n",
    "            for key, value in metadata.items():\n",
    "                if value is None:\n",
    "                    metadata[key] = \"Empty\"\n",
    "            \n",
    "            print(f\"Video title: {metadata['title']}\")\n",
    "\n",
    "            video_ext = metadata['ext']\n",
    "            initial_path = os.path.abspath(f'downloads/video.{video_ext}')\n",
    "            if not os.path.isfile(initial_path):\n",
    "                raise FileNotFoundError(f\"Downloaded video file not found: {initial_path}\")\n",
    "\n",
    "            counter = 1\n",
    "            final_path = os.path.abspath(f'downloads/video_{counter}.{video_ext}')\n",
    "            while os.path.isfile(final_path):\n",
    "                counter += 1\n",
    "                final_path = os.path.abspath(f'downloads/video_{counter}.{video_ext}')\n",
    "\n",
    "            os.rename(initial_path, final_path)\n",
    "            print(f\"Downloaded video saved to: {final_path}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            print(f\"Time taken for downloading video: {duration:.2f} seconds\")\n",
    "            \n",
    "            return final_path, metadata, duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading video: {e}\")\n",
    "        return None, None, 0\n",
    "\n",
    "# Function to extract audio from video\n",
    "def extract_audio(video_path):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio_path = video_path.replace('.mp4', '.wav')\n",
    "        audio_path = os.path.abspath(audio_path)\n",
    "        print(f\"Extracting audio to: {audio_path}\")\n",
    "        video.audio.write_audiofile(audio_path)\n",
    "        if not os.path.isfile(audio_path):\n",
    "            raise FileNotFoundError(f\"Extracted audio file not found: {audio_path}\")\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Audio extracted in {duration:.2f} seconds\")\n",
    "        return audio_path, duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "# Function to select Whisper model based on duration and mode\n",
    "def select_whisper_model(duration, mode='Fast'):\n",
    "    if mode == \"Accurate\":\n",
    "        if duration > 3600:  # More than 1 hour\n",
    "            model_name = \"tiny\"\n",
    "        elif duration > 1800:  # More than 30 minutes\n",
    "            model_name = \"base\"\n",
    "        elif duration > 600:  # More than 10 minutes\n",
    "            model_name = \"small\"\n",
    "        else:  # 10 minutes or less\n",
    "            model_name = \"medium\"\n",
    "    else:  # Fast mode\n",
    "        if duration > 1800:  # More than 30 minutes\n",
    "            model_name = \"tiny\"\n",
    "        elif duration > 600:  # More than 10 minutes\n",
    "            model_name = \"base\"\n",
    "        else:  # 10 minutes or less\n",
    "            model_name = \"small\"\n",
    "    \n",
    "    print(f\"Selected Whisper model: {model_name}\")\n",
    "    return model_name\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe_audio(audio_path, duration, mode='Fast'):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(f\"Transcribing audio from: {audio_path}\")\n",
    "        if not os.path.isfile(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "        model_name = select_whisper_model(duration, mode)\n",
    "        model = whisper.load_model(model_name)\n",
    "        \n",
    "        # Check if CUDA is available and use it\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(\"cuda\")\n",
    "            print(\"Using CUDA for Whisper transcription\")\n",
    "        \n",
    "        result = model.transcribe(audio_path)\n",
    "        end_time = time.time()\n",
    "        transcription_duration = end_time - start_time\n",
    "        print(f\"Transcription completed in {transcription_duration:.2f} seconds.\")\n",
    "        return result['text'], transcription_duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return \"\", 0\n",
    "\n",
    "# Function to transcribe audio with timestamps\n",
    "def transcribe_audio_with_timestamps(audio_path, duration, mode='Fast'):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(f\"Transcribing audio from: {audio_path}\")\n",
    "        if not os.path.isfile(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "        model_name = select_whisper_model(duration, mode)\n",
    "        model = whisper.load_model(model_name)\n",
    "        \n",
    "        # Check if CUDA is available and use it\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(\"cuda\")\n",
    "            print(\"Using CUDA for Whisper transcription\")\n",
    "        \n",
    "        result = model.transcribe(audio_path, word_timestamps=True)\n",
    "        end_time = time.time()\n",
    "        transcription_duration = end_time - start_time\n",
    "        print(f\"Transcription with timestamps completed in {transcription_duration:.2f} seconds.\")\n",
    "        return result, transcription_duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return {}, 0\n",
    "\n",
    "# Function to combine metadata and transcription\n",
    "def combine_metadata_and_transcription(metadata, transcription):\n",
    "    combined_text = \"Metadata:\\n\"\n",
    "    for key, value in metadata.items():\n",
    "        combined_text += f\"{key}: {value}\\n\"\n",
    "    combined_text += \"\\nTranscription:\\n\" + transcription\n",
    "    return combined_text\n",
    "\n",
    "# Function to combine metadata, transcription, and diarization\n",
    "def combine_metadata_transcription_diarization(metadata, transcription, diarization):\n",
    "    combined_text = \"Metadata:\\n\"\n",
    "    for key, value in metadata.items():\n",
    "        combined_text += f\"{key}: {value}\\n\"\n",
    "    combined_text += \"\\nDiarization:\\n\"\n",
    "    for segment in diarization.itertracks(yield_label=True):\n",
    "        speaker = segment[2]\n",
    "        start_time = segment[0].start\n",
    "        end_time = segment[0].end\n",
    "        combined_text += f\"Speaker {speaker} [{start_time:.2f} - {end_time:.2f}]: {transcription['segments'][0]['text']}\\n\"\n",
    "    return combined_text\n",
    "\n",
    "# Function to perform diarization\n",
    "def perform_diarization(audio_path, duration, mode='Fast'):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        pipeline = pipeline(\"speaker-diarization\")\n",
    "        \n",
    "        # Check if CUDA is available and use it\n",
    "        if torch.cuda.is_available():\n",
    "            pipeline.to(torch.device(\"cuda\"))\n",
    "            print(\"Using CUDA for Pyannote diarization\")\n",
    "        \n",
    "        diarization_result = pipeline(audio_path)\n",
    "        end_time = time.time()\n",
    "        diarization_duration = end_time - start_time\n",
    "        print(f\"Diarization completed in {diarization_duration:.2f} seconds.\")\n",
    "        return diarization_result, diarization_duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error during diarization: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "# Function to extract frames from video\n",
    "def extract_frames(video_path, num_frames=10):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_interval = total_frames // num_frames\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(0, total_frames, frame_interval):\n",
    "        video_capture.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = video_capture.read()\n",
    "        if ret:\n",
    "            timestamp = video_capture.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "            frames.append((frame, timestamp))\n",
    "    \n",
    "    video_capture.release()\n",
    "    return frames\n",
    "\n",
    "# Function to get text summaries from frames using GPT-4o-mini\n",
    "def summarize_frames(frames, api_key):\n",
    "    summaries = []\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    for frame, timestamp in frames:\n",
    "        image = Image.fromarray(frame)\n",
    "        base64_image = encode_image(image)\n",
    "        payload = {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 300\n",
    "        }\n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "        response_json = response.json()\n",
    "        if 'choices' in response_json:\n",
    "            summary = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "            summaries.append((summary, timestamp))\n",
    "        else:\n",
    "            print(f\"Error in API response: {response_json}\")\n",
    "            summaries.append((f\"Error: {response_json.get('error', 'Unknown error')}\", timestamp))\n",
    "    return summaries\n",
    "\n",
    "# Main function to process video\n",
    "def process_video(source_type, source, mode='Fast', process_type='Transcription', include_cv=False, api_key=None):\n",
    "    timings = {}\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    if source_type == 'upload':\n",
    "        video_path, filename, upload_duration = save_uploaded_file(source)\n",
    "        metadata = extract_metadata_ffmpeg(video_path)\n",
    "        timings['upload'] = upload_duration\n",
    "    elif source_type == 'youtube':\n",
    "        video_path, metadata, download_duration = download_youtube_video(source)\n",
    "        timings['download'] = download_duration\n",
    "    else:\n",
    "        print(\"Invalid source type.\")\n",
    "        return None, None\n",
    "    \n",
    "    if not video_path or not metadata:\n",
    "        print(\"Failed to get video or metadata.\")\n",
    "        return None, None\n",
    "    \n",
    "    audio_path, extract_duration = extract_audio(video_path)\n",
    "    if not audio_path:\n",
    "        print(\"Failed to extract audio.\")\n",
    "        return None, None\n",
    "    timings['extract_audio'] = extract_duration\n",
    "    \n",
    "    transcription_result, transcribe_duration = None, 0\n",
    "    diarization_result, diarization_duration = None, 0\n",
    "    if process_type == 'Transcription':\n",
    "        transcription_result, transcribe_duration = transcribe_audio(audio_path, metadata['duration'], mode)\n",
    "        if not transcription_result:\n",
    "            print(\"Failed to transcribe audio.\")\n",
    "            return None, None\n",
    "        timings['transcribe_audio'] = transcribe_duration\n",
    "    elif process_type == 'Diarization':\n",
    "        transcription_result, transcribe_duration = transcribe_audio_with_timestamps(audio_path, metadata['duration'], mode)\n",
    "        if not transcription_result:\n",
    "            print(\"Failed to transcribe audio with timestamps.\")\n",
    "            return None, None\n",
    "        timings['transcribe_audio_with_timestamps'] = transcribe_duration\n",
    "        \n",
    "        diarization_result, diarization_duration = perform_diarization(audio_path, metadata['duration'], mode)\n",
    "        if not diarization_result:\n",
    "            print(\"Failed to perform diarization.\")\n",
    "            return None, None\n",
    "        timings['diarization'] = diarization_duration\n",
    "    \n",
    "    visual_summaries = []\n",
    "    if include_cv and api_key:\n",
    "        frames = extract_frames(video_path)\n",
    "        visual_summaries = summarize_frames(frames, api_key)\n",
    "    \n",
    "    combined_text = \"\"\n",
    "    if process_type == 'Transcription':\n",
    "        combined_text = combine_metadata_and_transcription(metadata, transcription_result)\n",
    "    elif process_type == 'Diarization':\n",
    "        combined_text = combine_metadata_transcription_diarization(metadata, transcription_result, diarization_result)\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    timings['total_processing'] = total_end_time - total_start_time\n",
    "    \n",
    "    print(\"Timings:\", timings)\n",
    "    \n",
    "    document = Document(page_content=combined_text, metadata=metadata)\n",
    "    \n",
    "    visual_document = None\n",
    "    if include_cv and visual_summaries:\n",
    "        visual_combined_text = \"\\n\".join([f\"Time [{timestamp:.2f}]: {summary}\" for summary, timestamp in visual_summaries])\n",
    "        visual_document = Document(page_content=visual_combined_text, metadata=metadata)\n",
    "    \n",
    "        # Debugging: Print visual document contents\n",
    "        print(\"Visual Document Content:\", visual_document.page_content)\n",
    "    \n",
    "    # Clean up files\n",
    "    try:\n",
    "        os.remove(video_path)\n",
    "        os.remove(audio_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting files: {e}\")\n",
    "    \n",
    "    return document, visual_document\n",
    "\n",
    "# Function to create vectorstore\n",
    "def create_vectorstore(document):\n",
    "    try:\n",
    "        vectorstore = Chroma.from_documents(documents=[document], embedding=OpenAIEmbeddings())\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        return retriever\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vectorstore: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to create visual vectorstore\n",
    "def create_visual_vectorstore(visual_document):\n",
    "    try:\n",
    "        visual_vectorstore = Chroma.from_documents(documents=[visual_document], embedding=OpenAIEmbeddings())\n",
    "        visual_retriever = visual_vectorstore.as_retriever()\n",
    "        # Debugging: Validate vectorstore contents\n",
    "        print(\"Visual Vectorstore Contents:\", visual_vectorstore.similarity_search(\"What is in this image?\", k=1))\n",
    "        return visual_retriever\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visual vectorstore: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554df844-c758-46b7-a102-f07b6c15e522",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_button' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# https://www.youtube.com/watch?v=vr_FPbAHids\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprocess_button\u001b[49m\u001b[38;5;241m.\u001b[39mon_click(on_process_button_clicked)\n\u001b[0;32m      3\u001b[0m display(url_input, uploaded_file_widget, mode_selector, process_type_selector, process_button)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_button' is not defined"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=vr_FPbAHids\n",
    "process_button.on_click(on_process_button_clicked)\n",
    "display(url_input, uploaded_file_widget, mode_selector, process_type_selector, process_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14712d0-7783-4734-b534-570b440ad112",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ac8451-c18b-4c08-ad98-aa4263c654ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e647886eb5749c6a13408f6681e6738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='video/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b205e106114bc4a12529ec189e4f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='YouTube URL:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e647886eb5749c6a13408f6681e6738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='video/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9eb52e4cc74b48bd40d46bffa35d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Mode:', options=('Fast', 'Accurate'), value='Fast')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4319d6a3a042b89a266f772145da39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Process Type:', options=('Transcription', 'Diarization'), value='Transcription')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd79357a0982452fa0e7649bbb24e94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Include Computer Vision')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccc4adea7e5409cae5ca72509786118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process Video', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class VideoChatbot:\n",
    "    def __init__(self):\n",
    "        self.retriever = None\n",
    "        self.visual_retriever = None\n",
    "        self.qa_chain = None\n",
    "        self.visual_qa_chain = None\n",
    "        self.memory = MemorySaver()\n",
    "        self.model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        self.prompt = '''You are a chatbot that answers questions and performs tasks about a video that the user provides. \n",
    "                         Never ask the user to provide a video without first checking if there is one already.\n",
    "                         If lacking context, assume the user is always talking about the video.\n",
    "                         First, consider which tools you need to use, if any.\n",
    "                         When retrieving information, consider that the transcription might not be perfect every time.\n",
    "                         Then, if relevant, try to identify speakers by their names or usernames, using their dialogue and considering the available metadata.\n",
    "                         Then use more steps when needed in order to get the right answer. \n",
    "                         Finally, you must always identify the language the user is utilizing in their last message and answer in that language, unless the user tells you otherwise.\n",
    "                      '''\n",
    "        self.agent = None\n",
    "\n",
    "    def initialize_qa_chain(self):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "        try:\n",
    "            if self.retriever is not None:\n",
    "                qa = RetrievalQA.from_chain_type(\n",
    "                    llm=llm,\n",
    "                    chain_type=\"stuff\",\n",
    "                    retriever=self.retriever\n",
    "                )\n",
    "                self.qa_chain = qa\n",
    "                print(\"QA chain for transcript initialized successfully.\")\n",
    "            else:\n",
    "                print(\"Transcript retriever not available.\")\n",
    "            \n",
    "            if self.visual_retriever is not None:\n",
    "                visual_qa = RetrievalQA.from_chain_type(\n",
    "                    llm=llm,\n",
    "                    chain_type=\"stuff\",\n",
    "                    retriever=self.visual_retriever\n",
    "                )\n",
    "                self.visual_qa_chain = visual_qa\n",
    "                print(\"QA chain for visual summaries initialized successfully.\")\n",
    "            else:\n",
    "                print(\"Visual retriever not available.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing QA chains: {e}\")\n",
    "            self.qa_chain = None\n",
    "            self.visual_qa_chain = None\n",
    "\n",
    "    def create_agent(self):\n",
    "        tools = []\n",
    "        if self.qa_chain is not None:\n",
    "            tools.append(\n",
    "                Tool(\n",
    "                    name='video_transcript_retriever',\n",
    "                    func=self.qa_chain.run,\n",
    "                    description=(\n",
    "                        'Searches and returns excerpts from the transcript of the user uploaded video.'\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        if self.visual_qa_chain is not None:\n",
    "            tools.append(\n",
    "                Tool(\n",
    "                    name='video_visual_retriever',\n",
    "                    func=self.visual_qa_chain.run,\n",
    "                    description=(\n",
    "                        'Searches and returns visual context from the visual summaries of the user uploaded video.'\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        if tools:\n",
    "            self.agent = create_react_agent(self.model, tools=tools, messages_modifier=self.prompt, checkpointer=self.memory)\n",
    "            print(\"Agent created successfully.\")\n",
    "        else:\n",
    "            print(\"No tools available for the agent.\")\n",
    "\n",
    "    def process_query(self, query):\n",
    "        if not self.agent:\n",
    "            print(\"Agent not initialized.\")\n",
    "            return\n",
    "\n",
    "        inputs = {\"messages\": [(\"user\", query)]}\n",
    "        config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "        stream = self.agent.stream(inputs, config=config, stream_mode=\"values\")\n",
    "        for s in stream:\n",
    "            message = s[\"messages\"][-1]\n",
    "            if isinstance(message, tuple):\n",
    "                print(message)\n",
    "            else:\n",
    "                message.pretty_print()\n",
    "\n",
    "# Create instance of the chatbot\n",
    "chatbot = VideoChatbot()\n",
    "\n",
    "# Example usage of the chatbot after processing the video\n",
    "def on_process_button_clicked(b):\n",
    "    if url_input.value:\n",
    "        source_type = 'youtube'\n",
    "        source = url_input.value\n",
    "    elif uploaded_file_widget.value:\n",
    "        source_type = 'upload'\n",
    "        source = uploaded_file_widget\n",
    "    else:\n",
    "        print(\"Please provide a YouTube URL or upload a file.\")\n",
    "        return\n",
    "    \n",
    "    api_key = OPENAI_API_KEY  # Replace with your OpenAI API key\n",
    "    document, visual_document = process_video(source_type, source, mode_selector.value, process_type_selector.value, include_cv_selector.value, api_key)\n",
    "    if document:\n",
    "        try:\n",
    "            chatbot.retriever = create_vectorstore(document)\n",
    "            if include_cv_selector.value and visual_document:\n",
    "                chatbot.visual_retriever = create_visual_vectorstore(visual_document)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vectorstore: {e}\")\n",
    "            return\n",
    "    \n",
    "        if chatbot.retriever and (not include_cv_selector.value or chatbot.visual_retriever):\n",
    "            print(\"Vectorstores created and retrievers initialized.\")\n",
    "            chatbot.initialize_qa_chain()\n",
    "            chatbot.create_agent()\n",
    "            \n",
    "            # Test visual retriever independently\n",
    "            if chatbot.visual_retriever:\n",
    "                test_query = \"What is in this image?\"\n",
    "                test_result = chatbot.visual_retriever.get_relevant_documents(test_query)\n",
    "                print(\"Test Visual Retriever Result:\", test_result)\n",
    "            \n",
    "            example_query()  # Run example query after agent creation\n",
    "        else:\n",
    "            print(\"Failed to create vectorstores.\")\n",
    "    else:\n",
    "        print(\"Failed to process video.\")\n",
    "\n",
    "# Display widgets and process button\n",
    "uploaded_file_widget = handle_file_upload()\n",
    "url_input = widgets.Text(description=\"YouTube URL:\")\n",
    "mode_selector = widgets.Dropdown(options=[\"Fast\", \"Accurate\"], description=\"Mode:\")\n",
    "process_type_selector = widgets.Dropdown(options=[\"Transcription\", \"Diarization\"], description=\"Process Type:\")\n",
    "include_cv_selector = widgets.Checkbox(description=\"Include Computer Vision\", value=False)\n",
    "process_button = widgets.Button(description=\"Process Video\")\n",
    "process_button.on_click(on_process_button_clicked)\n",
    "display(url_input, uploaded_file_widget, mode_selector, process_type_selector, include_cv_selector, process_button)\n",
    "\n",
    "# Example query to the chatbot\n",
    "def example_query():\n",
    "    if chatbot.agent:\n",
    "        query = \"De qué va el vídeo?\"\n",
    "        chatbot.process_query(query)\n",
    "    else:\n",
    "        print(\"Agent not initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c1aef3-d826-4142-b92a-d8fa75fea5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Qué aspecto tienen los personajes?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  video_visual_retriever (call_AgkRrQDMmGpcLbfT4niSr9oE)\n",
      " Call ID: call_AgkRrQDMmGpcLbfT4niSr9oE\n",
      "  Args:\n",
      "    __arg1: personajes\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: video_visual_retriever\n",
      "\n",
      "Los personajes en el video \"When the healer thinks they're DPS\" de Viva La Dirt League son cuatro:\n",
      "\n",
      "1. **Britt**: Es uno de los tanques del grupo, cuya tarea principal es absorber el daño del enemigo.\n",
      "2. **Rowan**: Es el otro tanque del grupo, quien también se encarga de recibir el daño del enemigo.\n",
      "3. **Alan**: Es el arquero del grupo, encargado de atacar al enemigo desde la distancia.\n",
      "4. **Adam**: Es el sanador del grupo, quien debería enfocarse en curar a los compañeros de equipo, pero a veces se distrae intentando hacer daño.\n",
      "\n",
      "Están involucrados en una discusión sobre la importancia de que Adam se concentre en sanar en lugar de intentar hacer daño.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Los personajes en el video \"When the healer thinks they're DPS\" de Viva La Dirt League son:\n",
      "\n",
      "1. **Britt**: Es uno de los tanques del grupo, encargado de absorber el daño del enemigo.\n",
      "2. **Rowan**: Es el otro tanque del grupo, también responsable de recibir el daño.\n",
      "3. **Alan**: Es el arquero del grupo, que ataca al enemigo desde la distancia.\n",
      "4. **Adam**: Es el sanador del grupo, quien debería enfocarse en curar a sus compañeros, aunque a veces se distrae intentando hacer daño.\n",
      "\n",
      "Están vestidos con atuendos de fantasía medieval, adecuados para sus roles en el combate.\n"
     ]
    }
   ],
   "source": [
    "query = \"Qué aspecto tienen los personajes?\"\n",
    "chatbot.process_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da92ca-2182-44d0-9f4f-efb3c28d1b25",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12e992-5fee-458f-ab13-a9f3143d0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363cc3b5-b5f3-47f5-bb15-9ebcb89ee6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e3079-ca25-46bb-ba5c-ee01d9acb557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Dataset name\n",
    "dataset_name = \"Video_Test\"\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset = client.create_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3e12c-dd3d-4493-9b96-743e74df239c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"input_question\": \"Hello\"},\n",
    "        {\"input_question\": \"What is the video about?\"},\n",
    "        {\"input_question\": \"What was the last miniature that was released for the Thousand Sons?\"},\n",
    "        {\"input_question\": \"What is the most likely miniature to be released for Imperial Agents?\"},\n",
    "        {\"input_question\": \"What are the Space Wolves units that are less in need of an update?\"},\n",
    "        {\"input_question\": \"What can we expect the Black Templars to get if we are very optimistic?\"},\n",
    "        {\"input_question\": \"What is a common practice from Games Workshop when releasing a new codex?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"output_answer\": \"Hello, how can I help you?.\"},\n",
    "        {\"output_answer\": \"The video is about all the Warhammer 40K factions that are still missing a Codex in 10th edition and what novelties will the codexes bring when they are released, focusing on new possible miniatures.\"},\n",
    "        {\"output_answer\": \"The Infernal Master.\"},\n",
    "        {\"output_answer\": \"Inquisitor Coteaz.\"},\n",
    "        {\"output_answer\": \"Wulfen and Thunderwolves.\"},\n",
    "        {\"output_answer\": \"Some themed Terminators.\"},\n",
    "        {\"output_answer\": \"They release at least one miniature, usually one or two characters and often releasing Battleforces.\"},\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f184d-6cae-4dc0-9be4-f67a1a46f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_final_answer(messages):\n",
    "    \"\"\"\n",
    "    Extracts the content of the last AIMessage from the messages.\n",
    "    \n",
    "    Args:\n",
    "    messages (list): List of message dictionaries containing messages from human, AI, and tools.\n",
    "    \n",
    "    Returns:\n",
    "    str: The content of the last AIMessage.\n",
    "    \"\"\"\n",
    "    # Iterate over the messages in reverse order to find the last AIMessage\n",
    "    for message in reversed(messages):\n",
    "        # Check if the message is an instance of AIMessage\n",
    "        if isinstance(message, AIMessage):\n",
    "            return message.content\n",
    "    return ''\n",
    "\n",
    "\n",
    "def predict_rag_answer(example: dict):\n",
    "    #Use this for answer evaluation\n",
    "    query = example[\"input_question\"]\n",
    "    # Format inputs properly\n",
    "    inputs = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "    answer = agent.invoke(inputs, config=config, stream_mode=\"values\")\n",
    "    \n",
    "    if 'messages' in answer:\n",
    "        response = extract_final_answer(answer['messages'])\n",
    "    else:\n",
    "        response = \"No valid response found.\"\n",
    "    \n",
    "    return {\"answer\": response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb528fe5-c4ae-4d4c-a8de-5975171869e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Implement this if there is time, for evaluating correctly the retrieved documents and hallucinations.\n",
    "\n",
    "\"\"\"\n",
    "#Implement this if there is time, for evaluating correctly the retrieved documents and hallucinations.\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    #Use this for answer evaluation\n",
    "    query = example[\"input_question\"]\n",
    "    # Format inputs properly\n",
    "    inputs = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "    answer = agent.invoke(inputs, config=config, stream_mode=\"values\")\n",
    "    \n",
    "    if 'messages' in answer:\n",
    "        response = extract_final_answer(answer['messages'])\n",
    "    else:\n",
    "        response = \"No valid response found.\"\n",
    "    \n",
    "    return {\"answer\": response}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb1a5f-151c-4813-a644-5f0cb71bb296",
   "metadata": {},
   "source": [
    "### Response vs reference answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5809bb8-e2e1-4351-80d4-acdc86151da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    # Access example correctly\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    reference = example.outputs[\"output_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\n",
    "        \"question\": input_question,\n",
    "        \"correct_answer\": reference,\n",
    "        \"student_answer\": prediction\n",
    "    })\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180c3d2-f4bd-40e9-93dc-77ade961b2bd",
   "metadata": {},
   "source": [
    "### Response vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc90281-1dbf-4d1e-bb5f-f559f51009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "grade_prompt_answer_helpfulness = prompt = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer helpfulness\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da88a4b-e925-4712-a08c-117dc306ce84",
   "metadata": {},
   "source": [
    "### Response vs retrieved docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a238d6a-fc79-4f06-b7a3-e3d7cc23e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "grade_prompt_hallucinations = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    contexts = run.outputs.get(\"contexts\", [])\n",
    "\n",
    "    # RAG answer\n",
    "    prediction = run.outputs.get(\"answer\", \"No valid response found.\")\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb793cf2-9ff1-4f2c-bc1d-d0e81d590a46",
   "metadata": {},
   "source": [
    "### Retrieved docs vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61503b2-b1d9-45fa-a766-eefb40694206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    contexts = run.outputs.get(\"contexts\", [])\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5153cc1-091e-418c-8fb7-f5d82dfed7d2",
   "metadata": {},
   "source": [
    "## Run evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc6dc2-3788-4d03-ae06-f4361680c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        answer_evaluator,\n",
    "        answer_helpfulness_evaluator,\n",
    "        answer_hallucination_evaluator,\n",
    "        docs_relevance_evaluator\n",
    "    ],\n",
    "    experiment_prefix=\"Full_final_test\",\n",
    "    metadata={\"version\": \"Video_final_test, ChatMistralAI\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2a916-fdf6-4ea5-85af-666a6f72fe67",
   "metadata": {},
   "source": [
    "# Used package versions for requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8377adbc-6cf4-43f6-aa7c-2997f0aa976e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb==0.5.3\n",
      "ffmpeg-python==0.2.0\n",
      "flask==3.0.3\n",
      "ipython==8.15.0\n",
      "ipywidgets==8.1.3\n",
      "langchain==0.2.7\n",
      "langchain-community==0.2.6\n",
      "langchain-openai==0.1.16\n",
      "langgraph==0.1.8\n",
      "moviepy==1.0.3\n",
      "openai-whisper==20231117\n",
      "pyannote.audio==3.3.1\n",
      "python-dotenv==1.0.1\n",
      "torch==2.3.1\n",
      "whisper==1.1.10\n",
      "yt-dlp==2024.7.9\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# List of packages you want to check\n",
    "packages = [\n",
    "    'Flask', 'yt-dlp', 'moviepy', 'whisper', 'pyannote.audio', 'torch',\n",
    "    'ffmpeg-python', 'python-dotenv', 'langchain-openai', 'langchain',\n",
    "    'langgraph', 'ipywidgets', 'IPython', 'langchain-community', 'openai-whisper', 'chromadb'\n",
    "]\n",
    "\n",
    "# Get the installed version of each package\n",
    "installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set if pkg.key in [p.lower() for p in packages]}\n",
    "\n",
    "# Print the package versions\n",
    "for pkg, version in installed_packages.items():\n",
    "    print(f\"{pkg}=={version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cad64f7-661a-4fb6-ae35-e9f1614792f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Function to free GPU memory\n",
    "def free_gpu_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "# Example usage\n",
    "free_gpu_memory()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
