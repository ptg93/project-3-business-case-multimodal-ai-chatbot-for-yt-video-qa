{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42ec1a4-1cc7-4033-a663-df0f6717711e",
   "metadata": {},
   "source": [
    "# Import dependencies and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627db171-fcfa-4700-97c3-fa05945f8b18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import yt_dlp\n",
    "from moviepy.editor import VideoFileClip\n",
    "import whisper\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import ffmpeg\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.chains import (\n",
    "    create_retrieval_chain,\n",
    "    RetrievalQA,\n",
    ")\n",
    "\n",
    "from langgraph.checkpoint import MemorySaver\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langsmith.evaluation import evaluate\n",
    "from langchain import hub\n",
    "from langsmith import Client\n",
    "from langchain.schema import HumanMessage\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a60dbf0-1654-4c28-a786-4df9671abefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "ELEVEN_API_KEY = os.getenv('ELEVEN_API_KEY')\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
    "elif not LANGCHAIN_API_KEY:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY environment variable not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636d8cb0-0cc6-43e8-b123-61dfd65b0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Ironhack_Project3\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "\n",
    "# Set up directories\n",
    "os.makedirs('uploads', exist_ok=True)\n",
    "os.makedirs('downloads', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ce293-d998-402d-a92c-e19202a3db03",
   "metadata": {},
   "source": [
    "# Video-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511b4c69-19a8-4968-9688-03527ecb55cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to handle file uploads\n",
    "def handle_file_upload():\n",
    "    uploader = widgets.FileUpload(accept='video/*', multiple=False)\n",
    "    display(uploader)\n",
    "    return uploader\n",
    "\n",
    "# Function to save uploaded file\n",
    "def save_uploaded_file(file):\n",
    "    start_time = time.time()\n",
    "    if file.value:\n",
    "        for file_info in file.value:\n",
    "            filename = file_info['name']\n",
    "            content = file_info['content']\n",
    "            print(f\"Filename: {filename}\")\n",
    "            file_path = os.path.join('uploads', filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(content)\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            print(f\"File {filename} uploaded successfully to {file_path} in {duration:.2f} seconds\")\n",
    "            return file_path, filename, duration\n",
    "    else:\n",
    "        print(\"No file uploaded.\")\n",
    "        return None, None, 0\n",
    "\n",
    "# Function to extract metadata using ffmpeg\n",
    "def extract_metadata_ffmpeg(video_path):\n",
    "    try:\n",
    "        probe = ffmpeg.probe(video_path)\n",
    "        video_info = next(stream for stream in probe['streams'] if stream['codec_type'] == 'video')\n",
    "        metadata = {\n",
    "            \"duration\": float(video_info['duration']),\n",
    "            \"width\": int(video_info['width']),\n",
    "            \"height\": int(video_info['height']),\n",
    "            \"codec_name\": video_info['codec_name'],\n",
    "            \"bit_rate\": int(video_info['bit_rate'])\n",
    "        }\n",
    "        print(f\"Extracted metadata: {metadata}\")\n",
    "        return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata with ffmpeg: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to download video from YouTube\n",
    "def download_youtube_video(url):\n",
    "    start_time = time.time()\n",
    "    ydl_opts = {\n",
    "        'format': 'mp4',\n",
    "        'outtmpl': 'downloads/video.%(ext)s',\n",
    "        'verbose': True,\n",
    "    }\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info_dict = ydl.extract_info(url, download=True)\n",
    "            \n",
    "            metadata = {\n",
    "                \"title\": info_dict.get('title', 'video'),\n",
    "                \"id\": info_dict.get('id'),\n",
    "                \"duration\": info_dict.get('duration'),\n",
    "                \"upload_date\": info_dict.get('upload_date'),\n",
    "                \"uploader\": info_dict.get('uploader'),\n",
    "                \"uploader_id\": info_dict.get('uploader_id'),\n",
    "                \"view_count\": info_dict.get('view_count'),\n",
    "                \"like_count\": info_dict.get('like_count'),\n",
    "                \"dislike_count\": info_dict.get('dislike_count'),\n",
    "                \"average_rating\": info_dict.get('average_rating'),\n",
    "                \"age_limit\": info_dict.get('age_limit'),\n",
    "                \"categories\": \", \".join(info_dict.get('categories', [])),\n",
    "                \"tags\": \", \".join(info_dict.get('tags', [])),\n",
    "                \"ext\": info_dict.get('ext'),\n",
    "                \"thumbnail\": info_dict.get('thumbnail'),\n",
    "                \"description\": info_dict.get('description'),\n",
    "                \"channel\": info_dict.get('channel'),\n",
    "                \"channel_id\": info_dict.get('channel_id'),\n",
    "                \"is_live\": info_dict.get('is_live'),\n",
    "                \"release_date\": info_dict.get('release_date'),\n",
    "                \"availability\": info_dict.get('availability')\n",
    "            }\n",
    "            \n",
    "            for key, value in metadata.items():\n",
    "                if value is None:\n",
    "                    metadata[key] = \"Empty\"\n",
    "            \n",
    "            print(f\"Video title: {metadata['title']}\")\n",
    "\n",
    "            video_ext = metadata['ext']\n",
    "            initial_path = os.path.abspath(f'downloads/video.{video_ext}')\n",
    "            if not os.path.isfile(initial_path):\n",
    "                raise FileNotFoundError(f\"Downloaded video file not found: {initial_path}\")\n",
    "\n",
    "            counter = 1\n",
    "            final_path = os.path.abspath(f'downloads/video_{counter}.{video_ext}')\n",
    "            while os.path.isfile(final_path):\n",
    "                counter += 1\n",
    "                final_path = os.path.abspath(f'downloads/video_{counter}.{video_ext}')\n",
    "\n",
    "            os.rename(initial_path, final_path)\n",
    "            print(f\"Downloaded video saved to: {final_path}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            print(f\"Time taken for downloading video: {duration:.2f} seconds\")\n",
    "            \n",
    "            return final_path, metadata, duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading video: {e}\")\n",
    "        return None, None, 0\n",
    "\n",
    "# Function to extract audio from video\n",
    "def extract_audio(video_path):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio_path = video_path.replace('.mp4', '.wav')\n",
    "        audio_path = os.path.abspath(audio_path)\n",
    "        print(f\"Extracting audio to: {audio_path}\")\n",
    "        video.audio.write_audiofile(audio_path)\n",
    "        if not os.path.isfile(audio_path):\n",
    "            raise FileNotFoundError(f\"Extracted audio file not found: {audio_path}\")\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Audio extracted in {duration:.2f} seconds\")\n",
    "        return audio_path, duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "# Function to select Whisper model based on duration and mode\n",
    "def select_whisper_model(duration, mode='Fast'):\n",
    "    if mode == \"Accurate\":\n",
    "        if duration > 3600:  # More than 1 hour\n",
    "            model_name = \"tiny\"\n",
    "        elif duration > 1800:  # More than 30 minutes\n",
    "            model_name = \"base\"\n",
    "        elif duration > 600:  # More than 10 minutes\n",
    "            model_name = \"small\"\n",
    "        else:  # 10 minutes or less\n",
    "            model_name = \"medium\"\n",
    "    else:  # Fast mode\n",
    "        if duration > 1800:  # More than 30 minutes\n",
    "            model_name = \"tiny\"\n",
    "        elif duration > 600:  # More than 10 minutes\n",
    "            model_name = \"base\"\n",
    "        else:  # 10 minutes or less\n",
    "            model_name = \"small\"\n",
    "    \n",
    "    print(f\"Selected Whisper model: {model_name}\")\n",
    "    return model_name\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe_audio(audio_path, duration, mode='Fast'):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(f\"Transcribing audio from: {audio_path}\")\n",
    "        if not os.path.isfile(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "        model_name = select_whisper_model(duration, mode)\n",
    "        model = whisper.load_model(model_name)\n",
    "        \n",
    "        # Check if CUDA is available and use it\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(\"cuda\")\n",
    "            print(\"Using CUDA for Whisper transcription\")\n",
    "        \n",
    "        result = model.transcribe(audio_path)\n",
    "        end_time = time.time()\n",
    "        transcription_duration = end_time - start_time\n",
    "        print(f\"Transcription completed in {transcription_duration:.2f} seconds.\")\n",
    "        return result['text'], transcription_duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return \"\", 0\n",
    "\n",
    "# Function to transcribe audio with timestamps\n",
    "def transcribe_audio_with_timestamps(audio_path, duration, mode='Fast'):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(f\"Transcribing audio from: {audio_path}\")\n",
    "        if not os.path.isfile(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "        model_name = select_whisper_model(duration, mode)\n",
    "        model = whisper.load_model(model_name)\n",
    "        \n",
    "        # Check if CUDA is available and use it\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(\"cuda\")\n",
    "            print(\"Using CUDA for Whisper transcription\")\n",
    "        \n",
    "        result = model.transcribe(audio_path, word_timestamps=True)\n",
    "        end_time = time.time()\n",
    "        transcription_duration = end_time - start_time\n",
    "        print(f\"Transcription with timestamps completed in {transcription_duration:.2f} seconds.\")\n",
    "        return result, transcription_duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return {}, 0\n",
    "\n",
    "# Function to combine metadata and transcription\n",
    "def combine_metadata_and_transcription(metadata, transcription):\n",
    "    combined_text = \"Metadata:\\n\"\n",
    "    for key, value in metadata.items():\n",
    "        combined_text += f\"{key}: {value}\\n\"\n",
    "    combined_text += \"\\nTranscription:\\n\" + transcription\n",
    "    return combined_text\n",
    "\n",
    "# Function to combine metadata, transcription, and diarization\n",
    "def combine_metadata_transcription_diarization(metadata, transcription, diarization):\n",
    "    combined_text = \"Metadata:\\n\"\n",
    "    for key, value in metadata.items():\n",
    "        combined_text += f\"{key}: {value}\\n\"\n",
    "    combined_text += \"\\nDiarization:\\n\"\n",
    "    for segment in diarization.itertracks(yield_label=True):\n",
    "        speaker = segment[2]\n",
    "        start_time = segment[0].start\n",
    "        end_time = segment[0].end\n",
    "        combined_text += f\"Speaker {speaker} [{start_time:.2f} - {end_time:.2f}]: {transcription['segments'][0]['text']}\\n\"\n",
    "    return combined_text\n",
    "\n",
    "# Function to perform diarization\n",
    "def perform_diarization(audio_path, duration, mode='Fast'):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "        # Check if CUDA is available and use it\n",
    "        if torch.cuda.is_available():\n",
    "            pipeline.to(torch.device(\"cuda\"))\n",
    "            print(\"Using CUDA for Pyannote diarization\")\n",
    "        \n",
    "        diarization_result = pipeline(audio_path)\n",
    "        end_time = time.time()\n",
    "        diarization_duration = end_time - start_time\n",
    "        print(f\"Diarization completed in {diarization_duration:.2f} seconds.\")\n",
    "        return diarization_result, diarization_duration\n",
    "    except Exception as e:\n",
    "        print(f\"Error during diarization: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "# Main function to process video\n",
    "def process_video(source_type, source, mode='Fast', process_type='Transcription'):\n",
    "    timings = {}\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    if source_type == 'upload':\n",
    "        video_path, filename, upload_duration = save_uploaded_file(source)\n",
    "        metadata = extract_metadata_ffmpeg(video_path)\n",
    "        timings['upload'] = upload_duration\n",
    "    elif source_type == 'youtube':\n",
    "        video_path, metadata, download_duration = download_youtube_video(source)\n",
    "        timings['download'] = download_duration\n",
    "    else:\n",
    "        print(\"Invalid source type.\")\n",
    "        return None\n",
    "    \n",
    "    if not video_path or not metadata:\n",
    "        print(\"Failed to get video or metadata.\")\n",
    "        return None\n",
    "    \n",
    "    audio_path, extract_duration = extract_audio(video_path)\n",
    "    if not audio_path:\n",
    "        print(\"Failed to extract audio.\")\n",
    "        return None\n",
    "    timings['extract_audio'] = extract_duration\n",
    "    \n",
    "    if process_type == 'Transcription':\n",
    "        transcription, transcribe_duration = transcribe_audio(audio_path, metadata['duration'], mode)\n",
    "        if not transcription:\n",
    "            print(\"Failed to transcribe audio.\")\n",
    "            return None\n",
    "        timings['transcribe_audio'] = transcribe_duration\n",
    "        combined_text = combine_metadata_and_transcription(metadata, transcription)\n",
    "    elif process_type == 'Diarization':\n",
    "        transcription_result, transcribe_duration = transcribe_audio_with_timestamps(audio_path, metadata['duration'], mode)\n",
    "        if not transcription_result:\n",
    "            print(\"Failed to transcribe audio with timestamps.\")\n",
    "            return None\n",
    "        timings['transcribe_audio_with_timestamps'] = transcribe_duration\n",
    "        \n",
    "        diarization_result, diarization_duration = perform_diarization(audio_path, metadata['duration'], mode)\n",
    "        if not diarization_result:\n",
    "            print(\"Failed to perform diarization.\")\n",
    "            return None\n",
    "        timings['diarization'] = diarization_duration\n",
    "        \n",
    "        combined_text = combine_metadata_transcription_diarization(metadata, transcription_result, diarization_result)\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    timings['total_processing'] = total_end_time - total_start_time\n",
    "    \n",
    "    print(combined_text)\n",
    "    print(\"Timings:\", timings)\n",
    "    \n",
    "    document = Document(page_content=combined_text, metadata=metadata)\n",
    "    \n",
    "    # Clean up files\n",
    "    try:\n",
    "        os.remove(video_path)\n",
    "        os.remove(audio_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting files: {e}\")\n",
    "    \n",
    "    return document\n",
    "\n",
    "# Function to create vectorstore\n",
    "def create_vectorstore(document):\n",
    "    try:\n",
    "        vectorstore = Chroma.from_documents(documents=[document], embedding=OpenAIEmbeddings())\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        return retriever\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vectorstore: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "retriever = None\n",
    "\n",
    "def on_process_button_clicked(b):\n",
    "    global retriever\n",
    "    if url_input.value:\n",
    "        source_type = 'youtube'\n",
    "        source = url_input.value\n",
    "    elif uploaded_file_widget.value:\n",
    "        source_type = 'upload'\n",
    "        source = uploaded_file_widget\n",
    "    else:\n",
    "        print(\"Please provide a YouTube URL or upload a file.\")\n",
    "        return\n",
    "    \n",
    "    document = process_video(source_type, source, mode_selector.value, process_type_selector.value)\n",
    "    if document:\n",
    "        try:\n",
    "            vectorstore = Chroma.from_documents(documents=[document], embedding=OpenAIEmbeddings())\n",
    "            retriever = vectorstore.as_retriever()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vectorstore: {e}\")\n",
    "            exit()\n",
    "    \n",
    "        if retriever:\n",
    "            print(\"Vectorstore created and retriever initialized.\")\n",
    "        else:\n",
    "            print(\"Failed to create vectorstore.\")\n",
    "    else:\n",
    "        print(\"Failed to process video.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14712d0-7783-4734-b534-570b440ad112",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ac8451-c18b-4c08-ad98-aa4263c654ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44722906728045f2b5f9e99af58b29ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='video/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b924e1c0b24049abbb421b01b652e8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='YouTube URL:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44722906728045f2b5f9e99af58b29ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='video/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5268067a4f3c4c9f90e371bca94456a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Mode:', options=('Fast', 'Accurate'), value='Fast')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b4dea04c4d49cc8778c325305be04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Process Type:', options=('Transcription', 'Diarization'), value='Transcription')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe03f63db3a4bcc92d87bc533c08de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process Video', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the chatbot class\n",
    "class VideoChatbot:\n",
    "    def __init__(self):\n",
    "        self.retriever = None\n",
    "        self.qa_chain = None\n",
    "        self.memory = MemorySaver()\n",
    "        self.model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        self.prompt = '''You are a chatbot that answers questions and performs tasks about a video that the user provides. \n",
    "                         Never ask the user to provide a video without first checking if there is one already.\n",
    "                         If lacking context, assume the user is always talking about the video.\n",
    "                         First, consider which tools you need to use, if any.\n",
    "                         When retrieving information, consider that the transcription might not be perfect every time.\n",
    "                         Then, if relevant, try to identify speakers by their names or usernames, using their dialogue and considering the available metadata.\n",
    "                         Then use more steps when needed in order to get the right answer. \n",
    "                         Finally, you must always identify the language the user is utilizing in their last message and answer in that language, unless the user tells you otherwise.\n",
    "                      '''\n",
    "        self.agent = None\n",
    "\n",
    "    def initialize_qa_chain(self):\n",
    "        llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "        try:\n",
    "            qa = RetrievalQA.from_chain_type(\n",
    "                llm=llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=self.retriever\n",
    "            )\n",
    "            print(\"QA chain initialized successfully.\")\n",
    "            self.qa_chain = qa\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing QA chains: {e}\")\n",
    "            self.qa_chain = None\n",
    "\n",
    "    def create_agent(self):\n",
    "        tools = [\n",
    "            Tool(\n",
    "                name='video_transcript_retriever',\n",
    "                func=self.qa_chain.run,\n",
    "                description=(\n",
    "                    'Searches and returns excerpts from the transcript of the user uploaded video.'\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "        self.agent = create_react_agent(self.model, tools=tools, messages_modifier=self.prompt, checkpointer=self.memory)\n",
    "        print(\"Agent created successfully.\")\n",
    "\n",
    "    def process_query(self, query):\n",
    "        if not self.agent:\n",
    "            print(\"Agent not initialized.\")\n",
    "            return\n",
    "\n",
    "        inputs = {\"messages\": [(\"user\", query)]}\n",
    "        config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "        stream = self.agent.stream(inputs, config=config, stream_mode=\"values\")\n",
    "        for s in stream:\n",
    "            message = s[\"messages\"][-1]\n",
    "            if isinstance(message, tuple):\n",
    "                print(message)\n",
    "            else:\n",
    "                message.pretty_print()\n",
    "\n",
    "# Create instance of the chatbot\n",
    "chatbot = VideoChatbot()\n",
    "\n",
    "# Example usage of the chatbot after processing the video\n",
    "def on_process_button_clicked(b):\n",
    "    if url_input.value:\n",
    "        source_type = 'youtube'\n",
    "        source = url_input.value\n",
    "    elif uploaded_file_widget.value:\n",
    "        source_type = 'upload'\n",
    "        source = uploaded_file_widget\n",
    "    else:\n",
    "        print(\"Please provide a YouTube URL or upload a file.\")\n",
    "        return\n",
    "    \n",
    "    document = process_video(source_type, source, mode_selector.value, process_type_selector.value)\n",
    "    if document:\n",
    "        try:\n",
    "            vectorstore = Chroma.from_documents(documents=[document], embedding=OpenAIEmbeddings())\n",
    "            chatbot.retriever = vectorstore.as_retriever()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vectorstore: {e}\")\n",
    "            return\n",
    "    \n",
    "        if chatbot.retriever:\n",
    "            print(\"Vectorstore created and retriever initialized.\")\n",
    "            chatbot.initialize_qa_chain()\n",
    "            chatbot.create_agent()\n",
    "            example_query()  # Run example query after agent creation\n",
    "        else:\n",
    "            print(\"Failed to create vectorstore.\")\n",
    "    else:\n",
    "        print(\"Failed to process video.\")\n",
    "\n",
    "# Display widgets and process button\n",
    "uploaded_file_widget = handle_file_upload()\n",
    "url_input = widgets.Text(description=\"YouTube URL:\")\n",
    "mode_selector = widgets.Dropdown(options=[\"Fast\", \"Accurate\"], description=\"Mode:\")\n",
    "process_type_selector = widgets.Dropdown(options=[\"Transcription\", \"Diarization\"], description=\"Process Type:\")\n",
    "process_button = widgets.Button(description=\"Process Video\")\n",
    "process_button.on_click(on_process_button_clicked)\n",
    "display(url_input, uploaded_file_widget, mode_selector, process_type_selector, process_button)\n",
    "\n",
    "# Example query to the chatbot\n",
    "def example_query():\n",
    "    if chatbot.agent:\n",
    "        query = \"What is the video about?\"\n",
    "        chatbot.process_query(query)\n",
    "    else:\n",
    "        print(\"Agent not initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da76a8c3-e929-4c5a-9152-b632fa56d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the healer's punchline?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  video_transcript_retriever (call_RCB5pkuIRLGe81VrcH9WLaCI)\n",
      " Call ID: call_RCB5pkuIRLGe81VrcH9WLaCI\n",
      "  Args:\n",
      "    __arg1: healer punchline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: video_transcript_retriever\n",
      "\n",
      "The punchline of the video \"When the healer thinks they're DPS\" comes at the end when, despite being repeatedly told to focus on healing, Adam (the healer) starts the fight with a battle cry of \"Holy daga!\" and immediately dies. This leads the rest of the group to conclude, \"He's dead. He's already dead. He's dead. Should we go in? No. We don't have a healer. Let's go find another healer. Goodbye.\" This highlights the comedic frustration of the group trying to get their healer to do their assigned role.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The healer's punchline in the video is when Adam, despite being told to focus on healing, starts the fight with a battle cry of \"Holy daga!\" and immediately dies. This leads the group to humorously conclude, \"He's dead. He's already dead. He's dead. Should we go in? No. We don't have a healer. Let's go find another healer. Goodbye.\"\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the healer's punchline?\"\n",
    "chatbot.process_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da92ca-2182-44d0-9f4f-efb3c28d1b25",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310645da-d8b6-42a4-8cd5-2f76b8f6f906",
   "metadata": {},
   "source": [
    "We are going to build 2 evaluation sets: for Q&A and for summarization, 2 of the main tasks the model is going to execute. They have to be oriented for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e01515-b3cb-46fe-b9d2-9b56cccc1263",
   "metadata": {},
   "source": [
    "In order to get them right, we will use LangSmith https://docs.smith.langchain.com/tutorials/Developers/rag#evaluating-intermediate-steps and DeepEval https://docs.confident-ai.com/docs/guides-rag-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57e429-2e25-4fd6-acac-c444c5c6f94b",
   "metadata": {},
   "source": [
    "We first need to create our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec12e992-5fee-458f-ab13-a9f3143d0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e3079-ca25-46bb-ba5c-ee01d9acb557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Dataset name\n",
    "dataset_name = \"Video_Test\"\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset = client.create_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3e12c-dd3d-4493-9b96-743e74df239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(\n",
    "    inputs=[\n",
    "        {\"input_question\": \"Hello\"},\n",
    "        {\"input_question\": \"What is the video about?\"},\n",
    "        {\"input_question\": \"What was the last miniature that was released for the Thousand Sons?\"},\n",
    "        {\"input_question\": \"What is the most likely miniature to be released for Imperial Agents?\"},\n",
    "        {\"input_question\": \"What are the Space Wolves units that are less in need of an update?\"},\n",
    "        {\"input_question\": \"What can we expect the Black Templars to get if we are very optimistic?\"},\n",
    "        {\"input_question\": \"What is a common practice from Games Workshop when releasing a new codex?\"},\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"output_answer\": \"Hello, how can I help you?.\"},\n",
    "        {\"output_answer\": \"The video is about all the Warhammer 40K factions that are still missing a Codex in 10th edition and what novelties will the codexes bring when they are released, focusing on new possible miniatures.\"},\n",
    "        {\"output_answer\": \"The Infernal Master.\"},\n",
    "        {\"output_answer\": \"Inquisitor Coteaz.\"},\n",
    "        {\"output_answer\": \"Wulfen and Thunderwolves.\"},\n",
    "        {\"output_answer\": \"Some themed Terminators.\"},\n",
    "        {\"output_answer\": \"They release at least one miniature, usually one or two characters and often releasing Battleforces.\"},\n",
    "    ],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f184d-6cae-4dc0-9be4-f67a1a46f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(messages):\n",
    "    \"\"\"\n",
    "    Extracts the content of the last AIMessage from the messages.\n",
    "    \n",
    "    Args:\n",
    "    messages (list): List of message dictionaries containing messages from human, AI, and tools.\n",
    "    \n",
    "    Returns:\n",
    "    str: The content of the last AIMessage.\n",
    "    \"\"\"\n",
    "    # Iterate over the messages in reverse order to find the last AIMessage\n",
    "    for message in reversed(messages):\n",
    "        # Check if the message is an instance of AIMessage\n",
    "        if isinstance(message, AIMessage):\n",
    "            return message.content\n",
    "    return ''\n",
    "\n",
    "\n",
    "def predict_rag_answer(example: dict):\n",
    "    #Use this for answer evaluation\n",
    "    query = example[\"input_question\"]\n",
    "    # Format inputs properly\n",
    "    inputs = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "    answer = agent.invoke(inputs, config=config, stream_mode=\"values\")\n",
    "    \n",
    "    if 'messages' in answer:\n",
    "        response = extract_final_answer(answer['messages'])\n",
    "    else:\n",
    "        response = \"No valid response found.\"\n",
    "    \n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb528fe5-c4ae-4d4c-a8de-5975171869e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Implement this if there is time, for evaluating correctly the retrieved documents and hallucinations.\n",
    "\n",
    "\"\"\"\n",
    "#Implement this if there is time, for evaluating correctly the retrieved documents and hallucinations.\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    #Use this for answer evaluation\n",
    "    query = example[\"input_question\"]\n",
    "    # Format inputs properly\n",
    "    inputs = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "    answer = agent.invoke(inputs, config=config, stream_mode=\"values\")\n",
    "    \n",
    "    if 'messages' in answer:\n",
    "        response = extract_final_answer(answer['messages'])\n",
    "    else:\n",
    "        response = \"No valid response found.\"\n",
    "    \n",
    "    return {\"answer\": response}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb1a5f-151c-4813-a644-5f0cb71bb296",
   "metadata": {},
   "source": [
    "### Response vs reference answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5809bb8-e2e1-4351-80d4-acdc86151da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    # Access example correctly\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    reference = example.outputs[\"output_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\n",
    "        \"question\": input_question,\n",
    "        \"correct_answer\": reference,\n",
    "        \"student_answer\": prediction\n",
    "    })\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180c3d2-f4bd-40e9-93dc-77ade961b2bd",
   "metadata": {},
   "source": [
    "### Response vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc90281-1dbf-4d1e-bb5f-f559f51009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "grade_prompt_answer_helpfulness = prompt = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer helpfulness\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da88a4b-e925-4712-a08c-117dc306ce84",
   "metadata": {},
   "source": [
    "### Response vs retrieved docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a238d6a-fc79-4f06-b7a3-e3d7cc23e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "grade_prompt_hallucinations = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    contexts = run.outputs.get(\"contexts\", [])\n",
    "\n",
    "    # RAG answer\n",
    "    prediction = run.outputs.get(\"answer\", \"No valid response found.\")\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb793cf2-9ff1-4f2c-bc1d-d0e81d590a46",
   "metadata": {},
   "source": [
    "### Retrieved docs vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61503b2-b1d9-45fa-a766-eefb40694206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    contexts = run.outputs.get(\"contexts\", [])\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5153cc1-091e-418c-8fb7-f5d82dfed7d2",
   "metadata": {},
   "source": [
    "## Run evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc6dc2-3788-4d03-ae06-f4361680c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        answer_evaluator,\n",
    "        answer_helpfulness_evaluator,\n",
    "        answer_hallucination_evaluator,\n",
    "        docs_relevance_evaluator\n",
    "    ],\n",
    "    experiment_prefix=\"Full_final_test\",\n",
    "    metadata={\"version\": \"Video_final_test, ChatMistralAI\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2a916-fdf6-4ea5-85af-666a6f72fe67",
   "metadata": {},
   "source": [
    "# Used package versions for requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8377adbc-6cf4-43f6-aa7c-2997f0aa976e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg-python==0.2.0\n",
      "flask==3.0.3\n",
      "ipython==8.15.0\n",
      "ipywidgets==8.1.3\n",
      "langchain==0.2.7\n",
      "langchain-community==0.2.6\n",
      "langchain-openai==0.1.16\n",
      "langgraph==0.1.8\n",
      "moviepy==1.0.3\n",
      "pyannote.audio==3.3.1\n",
      "python-dotenv==1.0.1\n",
      "torch==2.3.1\n",
      "whisper==1.1.10\n",
      "yt-dlp==2024.7.9\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# List of packages you want to check\n",
    "packages = [\n",
    "    'Flask', 'yt-dlp', 'moviepy', 'whisper', 'pyannote.audio', 'torch',\n",
    "    'ffmpeg-python', 'python-dotenv', 'langchain-openai', 'langchain',\n",
    "    'langgraph', 'ipywidgets', 'IPython', 'langchain-community'\n",
    "]\n",
    "\n",
    "# Get the installed version of each package\n",
    "installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set if pkg.key in [p.lower() for p in packages]}\n",
    "\n",
    "# Print the package versions\n",
    "for pkg, version in installed_packages.items():\n",
    "    print(f\"{pkg}=={version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad64f7-661a-4fb6-ae35-e9f1614792f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
